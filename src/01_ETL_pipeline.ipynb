{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be36f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /home/codespace/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (0.129.0)\n",
      "Requirement already satisfied: uvicorn in /home/codespace/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (0.41.0)\n",
      "Requirement already satisfied: pydantic in /home/codespace/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv in /home/codespace/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (2.32.5)\n",
      "Requirement already satisfied: pytest in /home/codespace/.local/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (9.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from -r ../requirements.txt (line 10)) (2.4.2)\n",
      "Requirement already satisfied: starlette<1.0.0,>=0.40.0 in /home/codespace/.local/lib/python3.12/site-packages (from fastapi->-r ../requirements.txt (line 1)) (0.52.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from fastapi->-r ../requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/codespace/.local/lib/python3.12/site-packages (from fastapi->-r ../requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from fastapi->-r ../requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/codespace/.local/lib/python3.12/site-packages (from starlette<1.0.0,>=0.40.0->fastapi->-r ../requirements.txt (line 1)) (4.12.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<1.0.0,>=0.40.0->fastapi->-r ../requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: click>=7.0 in /home/codespace/.local/lib/python3.12/site-packages (from uvicorn->-r ../requirements.txt (line 2)) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/codespace/.local/lib/python3.12/site-packages (from uvicorn->-r ../requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic->-r ../requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic->-r ../requirements.txt (line 3)) (2.41.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->-r ../requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->-r ../requirements.txt (line 5)) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->-r ../requirements.txt (line 5)) (2025.11.12)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from pytest->-r ../requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: packaging>=22 in /home/codespace/.local/lib/python3.12/site-packages (from pytest->-r ../requirements.txt (line 6)) (26.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from pytest->-r ../requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /home/codespace/.local/lib/python3.12/site-packages (from pytest->-r ../requirements.txt (line 6)) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->-r ../requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 9)) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee3725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa87684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 850 rows from ../data/reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# --- EXTRACT: Load CSV with error handling ---\n",
    "CSV_PATH = \"../data/reviews.csv\"\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        raise FileNotFoundError(f\"CSV file not found at: {CSV_PATH}\")\n",
    "    dataset = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Successfully loaded {len(dataset)} rows from {CSV_PATH}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    raise\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"ERROR: Failed to parse CSV — {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Unexpected error reading CSV — {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9c0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. NULL VALUES\n",
      "============================================================\n",
      "\n",
      "Total rows: 850\n",
      "\n",
      "Null counts per column:\n",
      "No null values found.\n",
      "\n",
      "Total nulls: 0\n",
      "\n",
      "============================================================\n",
      "2. DUPLICATE ENTRIES\n",
      "============================================================\n",
      "\n",
      "Fully duplicate rows: 0\n",
      "Duplicate review_ids: 0\n",
      "\n",
      "============================================================\n",
      "3. TYPE MISMATCHES\n",
      "============================================================\n",
      "\n",
      "Column dtypes:\n",
      "review_id              int64\n",
      "product_id             int64\n",
      "product_name             str\n",
      "brand                    str\n",
      "category                 str\n",
      "price                float64\n",
      "customer_id            int64\n",
      "customer_name            str\n",
      "customer_email           str\n",
      "customer_age           int64\n",
      "customer_country         str\n",
      "customer_city            str\n",
      "rating                 int64\n",
      "review_title             str\n",
      "review_text              str\n",
      "review_date              str\n",
      "verified_purchase      int64\n",
      "helpful_votes          int64\n",
      "dtype: object\n",
      "\n",
      "--- Checking for type issues ---\n",
      "Non-numeric price values: 0\n",
      "Non-numeric rating values: 0\n",
      "Non-numeric customer_age values: 0\n",
      "Invalid date values: 0\n",
      "Unique verified_purchase values: [0 1]\n",
      "Non-numeric helpful_votes values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. NULL VALUES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal rows: {len(dataset)}\")\n",
    "print(f\"\\nNull counts per column:\")\n",
    "null_counts = dataset.isnull().sum()\n",
    "print(null_counts[null_counts > 0] if null_counts.sum() > 0 else \"No null values found.\")\n",
    "print(f\"\\nTotal nulls: {dataset.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. DUPLICATE ENTRIES\")\n",
    "print(\"=\" * 60)\n",
    "full_dupes = dataset.duplicated().sum()\n",
    "print(f\"\\nFully duplicate rows: {full_dupes}\")\n",
    "id_dupes = dataset.duplicated(subset=[\"review_id\"]).sum()\n",
    "print(f\"Duplicate review_ids: {id_dupes}\")\n",
    "if id_dupes > 0:\n",
    "    dup_ids = dataset[dataset.duplicated(subset=[\"review_id\"], keep=False)].sort_values(\"review_id\")\n",
    "    print(dup_ids[[\"review_id\", \"product_id\", \"customer_id\", \"rating\", \"review_date\"]])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. TYPE MISMATCHES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(dataset.dtypes)\n",
    "\n",
    "# Check specific columns for unexpected types\n",
    "print(\"\\n--- Checking for type issues ---\")\n",
    "# Price should be numeric\n",
    "non_numeric_price = pd.to_numeric(dataset[\"price\"], errors=\"coerce\").isna() & dataset[\"price\"].notna()\n",
    "print(f\"Non-numeric price values: {non_numeric_price.sum()}\")\n",
    "if non_numeric_price.sum() > 0:\n",
    "    print(dataset.loc[non_numeric_price, [\"review_id\", \"price\"]])\n",
    "\n",
    "# Rating should be numeric (1-5)\n",
    "non_numeric_rating = pd.to_numeric(dataset[\"rating\"], errors=\"coerce\").isna() & dataset[\"rating\"].notna()\n",
    "print(f\"Non-numeric rating values: {non_numeric_rating.sum()}\")\n",
    "if non_numeric_rating.sum() > 0:\n",
    "    print(dataset.loc[non_numeric_rating, [\"review_id\", \"rating\"]])\n",
    "\n",
    "# customer_age should be numeric\n",
    "non_numeric_age = pd.to_numeric(dataset[\"customer_age\"], errors=\"coerce\").isna() & dataset[\"customer_age\"].notna()\n",
    "print(f\"Non-numeric customer_age values: {non_numeric_age.sum()}\")\n",
    "if non_numeric_age.sum() > 0:\n",
    "    print(dataset.loc[non_numeric_age, [\"review_id\", \"customer_age\"]])\n",
    "\n",
    "# review_date should be parseable as date\n",
    "invalid_dates = pd.to_datetime(dataset[\"review_date\"], errors=\"coerce\").isna() & dataset[\"review_date\"].notna()\n",
    "print(f\"Invalid date values: {invalid_dates.sum()}\")\n",
    "if invalid_dates.sum() > 0:\n",
    "    print(dataset.loc[invalid_dates, [\"review_id\", \"review_date\"]])\n",
    "\n",
    "# verified_purchase should be 0/1\n",
    "print(f\"Unique verified_purchase values: {dataset['verified_purchase'].unique()}\")\n",
    "\n",
    "# helpful_votes should be numeric\n",
    "non_numeric_votes = pd.to_numeric(dataset[\"helpful_votes\"], errors=\"coerce\").isna() & dataset[\"helpful_votes\"].notna()\n",
    "print(f\"Non-numeric helpful_votes values: {non_numeric_votes.sum()}\")\n",
    "if non_numeric_votes.sum() > 0:\n",
    "    print(dataset.loc[non_numeric_votes, [\"review_id\", \"helpful_votes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1663006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before cleaning: (850, 18)\n",
      "[1] Missing values handled. Remaining nulls: 0\n",
      "[2] Text fields normalized (unicode, whitespace, casing).\n",
      "[3] All review_date values parsed successfully. Dtype: datetime64[us]\n",
      "[4] Duplicates removed: 0 full duplicates, 0 duplicate review_ids.\n",
      "\n",
      "Shape after cleaning: (850, 18)\n",
      "\n",
      "Cleaned dtypes:\n",
      "review_id                     int64\n",
      "product_id                    int64\n",
      "product_name                    str\n",
      "brand                           str\n",
      "category                        str\n",
      "price                       float64\n",
      "customer_id                   int64\n",
      "customer_name                   str\n",
      "customer_email                  str\n",
      "customer_age                  int64\n",
      "customer_country                str\n",
      "customer_city                   str\n",
      "rating                        int64\n",
      "review_title                    str\n",
      "review_text                     str\n",
      "review_date          datetime64[us]\n",
      "verified_purchase             int64\n",
      "helpful_votes                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape before cleaning: {dataset.shape}\")\n",
    "\n",
    "# --- 1. Handle Missing Values ---\n",
    "# Fill missing text fields with empty string, numeric with 0\n",
    "text_cols = dataset.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "num_cols = dataset.select_dtypes(include=[\"number\"]).columns\n",
    "dataset[text_cols] = dataset[text_cols].fillna(\"\")\n",
    "dataset[num_cols] = dataset[num_cols].fillna(0)\n",
    "print(f\"[1] Missing values handled. Remaining nulls: {dataset.isnull().sum().sum()}\")\n",
    "\n",
    "# --- 2. Normalize Text Fields ---\n",
    "# Strip whitespace, normalize unicode dashes/special chars, lowercase where appropriate\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize unicode characters (e.g. en-dash to hyphen) and strip whitespace.\"\"\"\n",
    "    if isinstance(s, str):\n",
    "        s = unicodedata.normalize(\"NFKC\", s)  # normalize unicode chars\n",
    "        s = s.strip()\n",
    "    return s\n",
    "\n",
    "# Apply normalization to all text columns\n",
    "for col in text_cols:\n",
    "    dataset[col] = dataset[col].apply(normalize_text)\n",
    "\n",
    "# Lowercase specific categorical/identifier text fields (not free-text like review_text/title)\n",
    "normalize_lower_cols = [\"customer_email\"]\n",
    "for col in normalize_lower_cols:\n",
    "    if col in dataset.columns:\n",
    "        dataset[col] = dataset[col].str.lower()\n",
    "\n",
    "# Title-case name fields\n",
    "name_cols = [\"product_name\", \"brand\", \"customer_name\", \"customer_country\", \"customer_city\", \"category\"]\n",
    "for col in name_cols:\n",
    "    if col in dataset.columns:\n",
    "        dataset[col] = dataset[col].str.strip().str.title()\n",
    "\n",
    "print(f\"[2] Text fields normalized (unicode, whitespace, casing).\")\n",
    "\n",
    "# --- 3. Parse and Validate Date Formats ---\n",
    "dataset[\"review_date\"] = pd.to_datetime(dataset[\"review_date\"], errors=\"coerce\")\n",
    "invalid_date_count = dataset[\"review_date\"].isna().sum()\n",
    "if invalid_date_count > 0:\n",
    "    print(f\"[3] WARNING: {invalid_date_count} rows with unparseable dates (set to NaT).\")\n",
    "else:\n",
    "    print(f\"[3] All review_date values parsed successfully. Dtype: {dataset['review_date'].dtype}\")\n",
    "\n",
    "# --- 4. Remove Duplicates ---\n",
    "before = len(dataset)\n",
    "dataset = dataset.drop_duplicates()\n",
    "after_full = len(dataset)\n",
    "dataset = dataset.drop_duplicates(subset=[\"review_id\"], keep=\"first\")\n",
    "after_id = len(dataset)\n",
    "print(f\"[4] Duplicates removed: {before - after_full} full duplicates, {after_full - after_id} duplicate review_ids.\")\n",
    "\n",
    "print(f\"\\nShape after cleaning: {dataset.shape}\")\n",
    "print(f\"\\nCleaned dtypes:\\n{dataset.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7dbb8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score statistics:\n",
      "count    850.000000\n",
      "mean       0.336828\n",
      "std        0.221072\n",
      "min       -0.375000\n",
      "25%        0.350000\n",
      "50%        0.412500\n",
      "75%        0.458333\n",
      "max        0.550000\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Sample rows:\n",
      "   review_id      product_name  rating  sentiment_score                                                                                                                                review_text\n",
      "0     900001     Beard Trimmer       3         0.458333                                                                     Met my needs. The irritation is fine, but the results could be better.\n",
      "1     900002   Design Handbook       3         0.354167                                                        It’s okay overall. The organization is fine, but the writing style could be better.\n",
      "2     900003       Moisturizer       5         0.460000        Exceeded my expectations. Packaging is excellent and the texture is better than expected. Shipping was fast and packaging was good.\n",
      "3     900004     Face Cleanser       5         0.433333                                                      Super easy to use. Irritation is excellent and the packaging is better than expected.\n",
      "4     900005   Vitamin C Serum       5         0.383333  Would definitely buy again. Irritation is excellent and the absorption is better than expected. Shipping was fast and packaging was good.\n",
      "5     900006   Sunscreen Spf50       5         0.460000         Exceeded my expectations. Scent is excellent and the absorption is better than expected. Shipping was fast and packaging was good.\n",
      "6     900007  Resistance Bands       5         0.350000                                                 Would definitely buy again. Stability is excellent and the weight is better than expected.\n",
      "7     900008   Camping Lantern       5         0.516667  Great value for the price. Portability is excellent and the durability is better than expected. Shipping was fast and packaging was good.\n",
      "8     900009          Cookbook       5         0.433333                                                Super easy to use. Organization is excellent and the print quality is better than expected.\n",
      "9     900010   Camping Lantern       4         0.412500                                                     Works exactly as described. Comfort is excellent and the grip is better than expected.\n"
     ]
    }
   ],
   "source": [
    "# === TRANSFORMATION: Calculate Sentiment Scores ===\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"Return polarity score in [-1.0, 1.0] using TextBlob.\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 0.0\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis on review_text\n",
    "dataset[\"sentiment_score\"] = dataset[\"review_text\"].apply(get_sentiment)\n",
    "\n",
    "print(\"Sentiment score statistics:\")\n",
    "print(dataset[\"sentiment_score\"].describe())\n",
    "print(f\"\\nSample rows:\")\n",
    "print(dataset[[\"review_id\", \"product_name\", \"rating\", \"sentiment_score\", \"review_text\"]].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cd5feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling average sentiment (window=3) calculated per product.\n",
      "\n",
      "Sample: 'Camping Lantern' (27 reviews)\n",
      " review_id    product_name review_date  rating  sentiment_score  rolling_avg_sentiment\n",
      "    900101 Camping Lantern  2024-02-27       5         0.350000               0.350000\n",
      "    900381 Camping Lantern  2024-05-03       5         0.460000               0.405000\n",
      "    900768 Camping Lantern  2024-05-08       3         0.458333               0.422778\n",
      "    900039 Camping Lantern  2024-06-02       3         0.422619               0.446984\n",
      "    900106 Camping Lantern  2024-07-14       5         0.438095               0.439683\n",
      "    900832 Camping Lantern  2024-08-12       5         0.550000               0.470238\n",
      "    900259 Camping Lantern  2024-09-11       5         0.460000               0.482698\n",
      "    900223 Camping Lantern  2024-11-06       5         0.412500               0.474167\n",
      "    900779 Camping Lantern  2024-11-09       4         0.412500               0.428333\n",
      "    900837 Camping Lantern  2024-12-11       4         0.383333               0.402778\n",
      "    900619 Camping Lantern  2024-12-29       5         0.433333               0.409722\n",
      "    900125 Camping Lantern  2025-01-24       3         0.354167               0.390278\n",
      "    900612 Camping Lantern  2025-05-12       4         0.438095               0.408532\n",
      "    900262 Camping Lantern  2025-05-24       3         0.255556               0.349272\n",
      "    900200 Camping Lantern  2025-06-23       3         0.458333               0.383995\n",
      "    900261 Camping Lantern  2025-07-17       4         0.383333               0.365741\n",
      "    900649 Camping Lantern  2025-07-20       3         0.458333               0.433333\n",
      "    900385 Camping Lantern  2025-08-08       4         0.412500               0.418056\n",
      "    900151 Camping Lantern  2025-08-09       4         0.350000               0.406944\n",
      "    900008 Camping Lantern  2025-09-06       5         0.516667               0.426389\n",
      "    900786 Camping Lantern  2025-09-23       4         0.350000               0.405556\n",
      "    900693 Camping Lantern  2025-11-02       5         0.550000               0.472222\n",
      "    900010 Camping Lantern  2025-11-04       4         0.412500               0.437500\n",
      "    900102 Camping Lantern  2025-11-07       5         0.466667               0.476389\n",
      "    900491 Camping Lantern  2026-01-25       2        -0.375000               0.168056\n",
      "    900753 Camping Lantern  2026-01-30       5         0.425000               0.172222\n",
      "    900047 Camping Lantern  2026-01-31       5         0.412500               0.154167\n",
      "\n",
      "--- Summary statistics for rolling_avg_sentiment ---\n",
      "count    850.000000\n",
      "mean       0.336838\n",
      "std        0.142018\n",
      "min       -0.375000\n",
      "25%        0.248810\n",
      "50%        0.386667\n",
      "75%        0.435278\n",
      "max        0.550000\n",
      "Name: rolling_avg_sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === TRANSFORMATION: Rolling Average Sentiment per Product ===\n",
    "\n",
    "# Sort by product and date for proper rolling calculation\n",
    "dataset = dataset.sort_values([\"product_id\", \"review_date\"]).reset_index(drop=True)\n",
    "\n",
    "# Calculate rolling average sentiment per product (window=3, min_periods=1)\n",
    "WINDOW_SIZE = 3\n",
    "\n",
    "dataset[\"rolling_avg_sentiment\"] = (\n",
    "    dataset.groupby(\"product_id\")[\"sentiment_score\"]\n",
    "    .transform(lambda x: x.rolling(window=WINDOW_SIZE, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "print(f\"Rolling average sentiment (window={WINDOW_SIZE}) calculated per product.\\n\")\n",
    "\n",
    "# Show a sample for one product\n",
    "sample_product = dataset[\"product_name\"].value_counts().index[0]\n",
    "sample = dataset[dataset[\"product_name\"] == sample_product][\n",
    "    [\"review_id\", \"product_name\", \"review_date\", \"rating\", \"sentiment_score\", \"rolling_avg_sentiment\"]\n",
    "]\n",
    "print(f\"Sample: '{sample_product}' ({len(sample)} reviews)\")\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "print(f\"\\n--- Summary statistics for rolling_avg_sentiment ---\")\n",
    "print(dataset[\"rolling_avg_sentiment\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07328e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] 'reviews' table: 850 rows written.\n",
      "[LOAD] 'product_rolling_sentiment' table: 850 rows written.\n",
      "\n",
      "Tables in database: ['reviews', 'product_rolling_sentiment']\n",
      "\n",
      "product_rolling_sentiment schema:\n",
      "                     name    type\n",
      "               product_id INTEGER\n",
      "             product_name    TEXT\n",
      "                   rating INTEGER\n",
      "rolling_average_sentiment    REAL\n",
      "                     date    TEXT\n",
      "  reviews: 850 rows\n",
      "  product_rolling_sentiment: 850 rows\n",
      "\n",
      "Database saved to: /workspaces/Sprints-capstone-project/data/reviews_db.sqlite\n"
     ]
    }
   ],
   "source": [
    "# === LOAD: Insert cleaned data into SQLite database ===\n",
    "import sqlite3\n",
    "\n",
    "DB_PATH = \"../data/reviews_db.sqlite\"\n",
    "\n",
    "try:\n",
    "    # --- 1. Write the full cleaned reviews table ---\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "    # Convert review_date to string for SQLite storage\n",
    "    load_df = dataset.copy()\n",
    "    load_df[\"review_date\"] = load_df[\"review_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    load_df.to_sql(\"reviews\", conn, if_exists=\"replace\", index=False)\n",
    "    print(f\"[LOAD] 'reviews' table: {len(load_df)} rows written.\")\n",
    "\n",
    "    # --- 2. Create product_rolling_sentiment table (schema matches app.py API) ---\n",
    "    # API expects: product_id, product_name, rating, rolling_average_sentiment, date\n",
    "    rolling_df = load_df[[\"product_id\", \"product_name\", \"rating\",\n",
    "                          \"rolling_avg_sentiment\", \"review_date\"]].copy()\n",
    "    rolling_df = rolling_df.rename(columns={\n",
    "        \"rolling_avg_sentiment\": \"rolling_average_sentiment\",\n",
    "        \"review_date\": \"date\"\n",
    "    })\n",
    "\n",
    "    rolling_df.to_sql(\"product_rolling_sentiment\", conn, if_exists=\"replace\", index=False)\n",
    "    print(f\"[LOAD] 'product_rolling_sentiment' table: {len(rolling_df)} rows written.\")\n",
    "\n",
    "    # --- 3. Verify tables ---\n",
    "    tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "    print(f\"\\nTables in database: {tables['name'].tolist()}\")\n",
    "\n",
    "    # Verify schema of product_rolling_sentiment\n",
    "    schema = pd.read_sql(\"PRAGMA table_info(product_rolling_sentiment)\", conn)\n",
    "    print(f\"\\nproduct_rolling_sentiment schema:\")\n",
    "    print(schema[[\"name\", \"type\"]].to_string(index=False))\n",
    "\n",
    "    # Quick row count check\n",
    "    for tbl in tables[\"name\"]:\n",
    "        count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {tbl}\", conn).iloc[0, 0]\n",
    "        print(f\"  {tbl}: {count} rows\")\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"\\nDatabase saved to: {os.path.abspath(DB_PATH)}\")\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"ERROR: SQLite error — {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Unexpected error during LOAD — {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
